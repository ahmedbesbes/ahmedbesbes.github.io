{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a follow-up to the <a href=\"http://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-1-an-overview-of-the-mrnet-dataset.html\">previous one</a> in which we explored the problem of ACL tears and the related MRNet dataset released by Stanford ML group. If you want to learn more about Stanford's work you can visit this <a href=\"https://stanfordmlgroup.github.io/projects/mrnet/\">link</a>.\n",
    "\n",
    "Today, we are going to use MRNet data to build a Convolutional Neural Network that detects and classifies ACL tears from MRI scans. We will implement it in **PyTorch** to fully take advantage of the capabilities of this framework. \n",
    "\n",
    "By the end of this post you will learn a few things:\n",
    "\n",
    "- How to write a convolutional neural network specifically designed to process MRI scans for a given plane (axial,sagittal and coronal): we will see that this architecture slightly differs from conventional cnn architectures that classify natural images\n",
    "- How to implement a \"meta-model\" that combines the predictions of the aforementioned cnn models trained individually to classify ACL injury on each plane\n",
    "- How to handle the lack of MRI data using transfer learning and data augmentation\n",
    "- How to build an end to end PyTorch training pipeline to load and process data, train, monitor and evaluate the models\n",
    "\n",
    "If you reach the end of this article, you should have a global overview of the ACL tear classification problem. \n",
    "\n",
    "**Note: if this medical task doesn’t interest you in particular, you can reuse some blocks for other medical imaging tasks such as tumor classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder of the problem\n",
    "\n",
    "This chart summarizes the problem and the way we're going to address it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/article_7/overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it, each patient has three MRI scans taken with respect to three different planes: axial, sagittal and coronal.\n",
    "\n",
    "We are going to build 3 independent CNN models that allow to classify ACL tear per plane.\n",
    "\n",
    "Each of these networks will specialize in detecting ACL tear from a given plane. In order to have a model that performs well everywhere, we will combine these three models using a **stacking operation.**\n",
    "\n",
    "In practice, we are going to train a logistic regression on the CNNs probability outputs to predict ACL tear.\n",
    "\n",
    "**This is an important step that is supposed to mimic the way radiologists consider different MRI scans (in different planes) of a single patient in order to make a robust diagnostic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRNet architecture\n",
    "\n",
    "Let's now detail the CNN model architecture that will be common to the three networks. The Stanford team named it the **MRNet**.\n",
    "\n",
    "The MRNet is a convolutional neural network that takes as input an MRI scan and outputs a classification prediction, namely an ACL tear probability.\n",
    "\n",
    "The input has dimensions s x 3 x 256 x 256 where s is the number of slices (i.e. images) in the MRI scan. 3 is number of color channel per slice. \n",
    "\n",
    "First, each MRI slice is passed through a feature extractor based on a pre-trained AlexNet to obtain a s x 256 x 7 x 7 tensor containing features for each slice. A global average pooling layer is then applied to reduce these features to s x 256. Basically, each 7x7 matrix is reduced to its mean.\n",
    "\n",
    "Then max pooling across slices is applied to obtain a 256-dimensional vector which is passed a fully connected layer and sigmoid activation function to obtain a prediction between 0 an 1.\n",
    "\n",
    "The chart below, taken from Stanford's paper, illustrates this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/article_7/mrnet_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points to notice about this architecture:\n",
    "\n",
    "- Given that s varies from a patient to another, it is impossible to stack piles of MRI in batches. Therefore, we will use **batches of size 1** during training\n",
    "\n",
    "- Slices are fed in parallel in AlexNet the same way batches of natural images are processed in parallel by this pretrained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the model is done through the minimization of the cross-entropy loss using Adam optimizer.\n",
    "\n",
    "To take into account the imbalanced nature of the classes, the loss of an example was scaled inversely proportionally to the prevalence of that example’s class in the dataset in order to penalize the error more on the least present examples. \n",
    "\n",
    "During training, the gradient of the loss is computed on each training example using the backpropagation agorithm and the network's parameters are then adjusted in the opposite direction of the gradient.\n",
    "\n",
    "During training, some geometric transformations are applied on the input MRI. These transformations are label-invariant. They are meant to bring diversity in the dataset and increase the stability of the model while decrease its tendency to overfitting. This procedure is called **data augmentation**.\n",
    "\n",
    "We'll sequentially apply 3 geometric transformations on each input MRI. \n",
    "\n",
    "- Random rotation between -25 and 25 degrees\n",
    "- Random shift in both direction between -25 and 25 pixels\n",
    "- Random horizontal flip with 50% probability\n",
    "\n",
    "Note that data augmentation is done identically over all the slices of an MRI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code structure\n",
    "\n",
    "Let's now turn what we've just seen into a PyTorch implementation.\n",
    "\n",
    "We will organize the source code in three main files (all the code available on <a href=\"https://github.com/ahmedbesbes/mrnet\">github</a> ):\n",
    "\n",
    "- model.py\n",
    "- dataloader.py\n",
    "- train.py \n",
    "\n",
    "The chart below summarizes pretty much the responsibility of each script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/article_7/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- The model architecture: model.py\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = models.alexnet(pretrained=True)\n",
    "        self.pooling_layer = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x, dim=0) \n",
    "        features = self.pretrained_model.features(x)\n",
    "        pooled_features = self.pooling_layer(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        flattened_features = torch.max(pooled_features, 0, keepdim=True)[0]\n",
    "        output = self.classifer(flattened_features)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is actually quite simple. We define it as a class called MRNet that inherits from the torch.nn.Module class.\n",
    "\n",
    "In the constructor we define three objects: \n",
    "\n",
    "- the pretrained AlexNet model\n",
    "- the pooling layer\n",
    "- the dense layer that acts as a classification layer\n",
    "\n",
    "In the forward method, we actually write the forward pass, i.e. the operations the network performs on the input until it computes the predictions.\n",
    "\n",
    "Let's detail it step by step:\n",
    "\n",
    "- this method recieves an input x of shape (1, s, 256, 256, 3) since, as we said earlier, we are dealing with batches of size 1\n",
    "\n",
    "- it removes the first dimension by \"squeezing\" the input and turning its shape to (s, 256, 256, 3)\n",
    "\n",
    "- now (s, 256, 256, 3) is a regular tensor shape that can be fed to an AlexNet which produces the features of shape (s, 256, 7, 7) afterwards\n",
    "\n",
    "- the features are pooled which produces an output of shape (s, 256)\n",
    "\n",
    "- the pooled features are flattened in a 256 dimension vector that is finally fed to the classifier that outputs a scalar value. Note that we don't use a sigmoid activation here. Sigmoid is applied in the loss direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - The custom dataset: dataloader.py\n",
    "\n",
    "In this script, we define a custom Dataset object that loads the MRNet data in the main program.\n",
    "\n",
    "To create the dataset, we define a class called MRDataset that inherits from the class torch.utils.data.Dataset\n",
    "\n",
    "```python\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, root_dir, task, plane, train=True, transform=None, weights=None):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.plane = plane\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "        else:\n",
    "            transform = None\n",
    "            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "\n",
    "        self.records['id'] = self.records['id'].map(\n",
    "            lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "        self.paths = [self.folder_path + filename +\n",
    "                      '.npy' for filename in self.records['id'].tolist()]\n",
    "        self.labels = self.records['label'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "        if weights is None:\n",
    "            pos = np.sum(self.labels)\n",
    "            neg = len(self.labels) - pos\n",
    "            self.weights = [1, neg / pos]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        array = np.load(self.paths[index])\n",
    "        label = self.labels[index]\n",
    "        label = torch.FloatTensor([label])\n",
    "        \n",
    "        if self.transform:\n",
    "            array = self.transform(array)\n",
    "        else:\n",
    "            array = np.stack((array,)*3, axis=1)\n",
    "            array = torch.FloatTensor(array)\n",
    "\n",
    "        if label.item() == 1:\n",
    "            weight = np.array([self.weights[1]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "        else:\n",
    "            weight = np.array([self.weights[0]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "\n",
    "        return array, label, weight\n",
    "\n",
    "```\n",
    "\n",
    "In the constructor of MRDataset, we define a set of arguments:\n",
    "\n",
    "- root_dir: ./data/\n",
    "- task: either acl, meniscus or abnormal. we'll focus on **acl** in this post\n",
    "- plane: either sagittal, coronal or axial\n",
    "- train: a boolean variable that indicates whether we are processing train data or not (validation)\n",
    "- transform: the series of data augmentation operations. If None, no data augmentation\n",
    "- weights: custom weights for each class (default to None): this is used to adjust the loss function. When None, weights are computed automatically.\n",
    "\n",
    "In the remaining part of the constructor, we prepare the paths, the labels, and the weights that correspond to each data sample.\n",
    "\n",
    "In the \\__len\\__ function, we return the length of the data \n",
    "\n",
    "In the \\__getitem\\__ function we return the MRI scan .npy file, the label and the weight after applying minor preprocessing and eventual data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Where the training happens: train.py\n",
    "\n",
    "This script is the main part of the application. It does the heavy lifting and outputs (i.e. saves) a trained model.\n",
    "\n",
    "Here is what it does in a nutshell:\n",
    "- It imports dataloader.py to load the data from both train or validation sets.\n",
    "- It imports model.py and instantiates an MRNet model before updating its weights.\n",
    "- It launches a training and validation loop over a given number of epochs.\n",
    "\n",
    "If we skip everything and look at the first line of code that is executed when the script is called from terminal,\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_arguments()\n",
    "    run(args)\n",
    "```\n",
    "\n",
    "... we'll see that it starts by loading some arguments that are given in the command line.\n",
    "\n",
    "```python\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-t', '--task', type=str, required=True,\n",
    "                        choices=['abnormal', 'acl', 'meniscus'])\n",
    "    parser.add_argument('-p', '--plane', type=str, required=True,\n",
    "                        choices=['sagittal', 'coronal', 'axial'])\n",
    "    parser.add_argument('--augment', type=int, choices=[0, 1], default=1)\n",
    "    parser.add_argument('--lr_scheduler', type=int, choices=[0, 1], default=1)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--flush_history', type=int, choices=[0, 1], default=0)\n",
    "    parser.add_argument('--save_model', type=int, choices=[0, 1], default=1)\n",
    "    parser.add_argument('--patience', type=int, choices=[0, 1], default=5)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the arguments are loaded in the **args** variable, the run function starts executing.\n",
    "\n",
    "Without going into much details, this function first starts by creating a folder that'll be used by tensorboard to save the training logs and visualize the metrics of the training session:\n",
    "\n",
    "On each run of the script, a new folder (named after the timestamp) is created:  \n",
    "\n",
    "```python\n",
    "def run(args):\n",
    "    log_root_folder = \"./logs/{0}/{1}/\".format(args.task, args.plane)\n",
    "    if args.flush_history == 1:\n",
    "        objects = os.listdir(log_root_folder)\n",
    "        for f in objects:\n",
    "            if os.path.isdir(log_root_folder + f):\n",
    "                shutil.rmtree(log_root_folder + f)\n",
    "\n",
    "    now = datetime.now()\n",
    "    logdir = log_root_folder + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "    writer = SummaryWriter(logdir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the data augmentation pipeline\n",
    "```python\n",
    "    # data augmentation pipeline\n",
    "    augmentor = Compose([\n",
    "        transforms.Lambda(lambda x: torch.Tensor(x)),\n",
    "        RandomRotate(25),\n",
    "        RandomTranslate([0.11, 0.11]),\n",
    "        RandomFlip(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1, 1).permute(1, 0, 2, 3)),\n",
    "    ])\n",
    "```\n",
    "\n",
    "and instantiate a train and validation MRDataset(s).  \n",
    "\n",
    "```python\n",
    "    train_dataset = MRDataset('./data/', args.task, args.plane, transform=augmentor, train=True)\n",
    "    validation_dataset = MRDataset('./data/', args.task, args.plane, train=False)\n",
    "    \n",
    "```\n",
    "\n",
    "These datasets are now passed to a Dataloader which is a handy PyTorch object that allows to efficiently iterate over the data by leveraging batching, shuffling, multiprocessing and data augmentation.\n",
    "\n",
    "```python\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=1, shuffle=True, num_workers=11, drop_last=False)\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=1, shuffle=-True, num_workers=11, drop_last=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the MRNet model and pass its paramaters to GPU.\n",
    "\n",
    "We define the Adam optimizer as well as a learning rate scheduler.\n",
    "\n",
    "We instantiate the number of epochs and the patience i.e. the minimum number of epochs without improvement of the loss.\n",
    "\n",
    "```python\n",
    "    mrnet = model.MRNet()\n",
    "    mrnet = mrnet.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(mrnet.parameters(), lr=1e-5, weight_decay=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=3, factor=.3, threshold=1e-4, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_auc = float(0)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    iteration_change_loss = 0\n",
    "    patience = args.patience\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the best part, the training part:\n",
    "\n",
    "```python\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_auc = train_model(\n",
    "            mrnet, train_loader, epoch, num_epochs, optimizer, writer)\n",
    "        val_loss, val_auc = evaluate_model(\n",
    "            mrnet, validation_loader, epoch, num_epochs, writer)\n",
    "\n",
    "        print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3}\".format(\n",
    "            train_loss, train_auc, val_loss, val_auc))\n",
    "\n",
    "        if args.lr_scheduler == 1:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        iteration_change_loss += 1\n",
    "        print('-' * 30)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            if bool(args.save_model):\n",
    "                file_name = f'model_{args.task}_{args.plane}_val_auc_\\\n",
    "                            {val_auc:0.4f}_train_auc_{train_auc:0.4f}\\\n",
    "                            _epoch_{epoch+1}.pth'\n",
    "                for f in os.listdir('./models/'):\n",
    "                    if (args.task in f) and (args.plane in f):\n",
    "                        os.remove(f'./models/{f}')\n",
    "                torch.save(mrnet, f'./models/{file_name}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            iteration_change_loss = 0\n",
    "\n",
    "        if iteration_change_loss == patience:\n",
    "            print('Early stopping after {0} iterations without the decrease of the val loss'.\n",
    "                  format(iteration_change_loss))\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each epoch, many things are executed:\n",
    "\n",
    "- We train the model using train_model\n",
    "- We evaluate the model using evaluate_model\n",
    "- We print the AUC metric and the loss on train and validation data\n",
    "- We let the learning rate scheduler update the learning rate\n",
    "- if the validation AUC improves we checkpoint the model to disk\n",
    "- if the number of epochs without improvement of the loss is higher than the patience, we interrupt the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on the train_model function that is called on each epoch.\n",
    "\n",
    "Here's the full code:\n",
    "\n",
    "```python\n",
    "def train_model(model, train_loader, epoch, num_epochs, optimizer, writer, log_every=100):\n",
    "    _ = model.train()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    losses = []\n",
    "\n",
    "    for i, (image, label, weight) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            weight = weight.cuda()\n",
    "\n",
    "        prediction = model.forward(image.float())\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            prediction[0], label[0], weight=weight[0])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.sigmoid(prediction).item()\n",
    "        y_true = int(label.item())\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "        y_trues.append(y_true)\n",
    "\n",
    "        try:\n",
    "            auc = metrics.roc_auc_score(y_trues, y_preds)\n",
    "        except:\n",
    "            auc = 0.5\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', loss_value,\n",
    "                          epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('Train/AUC', auc, epoch * len(train_loader) + i)\n",
    "\n",
    "        if (i % log_every == 0) & (i > 0):\n",
    "            print('''[Epoch: {0} / {1} |Single batch number : {2} / {3} ]\\\n",
    "                    | avg train loss {4} | train auc : {5}'''.\n",
    "                  format(\n",
    "                      epoch + 1,\n",
    "                      num_epochs,\n",
    "                      i,\n",
    "                      len(train_loader),\n",
    "                      np.round(np.mean(losses), 4),\n",
    "                      np.round(auc, 4)\n",
    "                  )\n",
    "                  )\n",
    "    writer.add_scalar('Train/AUC_epoch', auc, epoch + i)\n",
    "    train_loss_epoch = np.round(np.mean(losses), 4)\n",
    "    train_auc_epoch = np.round(auc, 4)\n",
    "    return train_loss_epoch, train_auc_epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it into pieces:\n",
    "\n",
    "We start by setting the model to train mode, then pass it to GPU and initialize the lists that will contain predictions, true labels and the losses on each individual sample.\n",
    "\n",
    "```python\n",
    "    _ = model.train()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    losses = []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we loop over the dataloader:\n",
    "\n",
    "At each step:\n",
    "\n",
    "- A single MRI scan and its corresponding label and weight are passed to GPU\n",
    "- The network computes a forward pass on the MRI scan which results in a prediciton\n",
    "- The loss between the prediction and the true label is computed\n",
    "- Backpropagation of the loss: computation of the gradients\n",
    "- Weights update by the optimizer\n",
    "\n",
    "\n",
    "```python \n",
    "    for i, (image, label, weight) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            weight = weight.cuda()\n",
    "            \n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            prediction[0], label[0], weight=weight[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining code executed on each epoch monitors the training metrics and logs them to Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: classification of ACL tears on sagittal plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup: Nvidia 1080 Ti and i7 8700K CPU.\n",
    "- Training took approximately 1h and 11 minutes. 35 epochs.\n",
    "- The best model is saved on disk with the following AUC scores \n",
    "\n",
    "    - On train set: 0.8669\n",
    "    - On validation set: 0.8850\n",
    "\n",
    "The AUC and the loss can be viewed in Tensorboard at each epoch and each batch:\n",
    "\n",
    "<img src=\"./images/article_7/sagittal_tensorboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a global ACL tear classifer\n",
    "\n",
    "Now that we've seen how to train an ACL tear classifier on the sagittal plane, we can follow the same procedure for the two other planes. Results are comparable.\n",
    "\n",
    "When the three models and trained and saved to disk, we use them to compute predictions on each sample of training and validation.\n",
    "\n",
    "```python\n",
    "def extract_predictions(task, plane, train=True):\n",
    "    assert task in ['acl', 'meniscus', 'abnormal']\n",
    "    assert plane in ['axial', 'coronal', 'sagittal']\n",
    "    \n",
    "    models = os.listdir('../models/')\n",
    "\n",
    "    model_name = list(filter(lambda name: task in name and plane in name, models))[0]\n",
    "    model_path = f'../models/{model_name}'\n",
    "\n",
    "    mrnet = torch.load(model_path)\n",
    "    _ = mrnet.eval()\n",
    "    \n",
    "    train_dataset = MRDataset('../data/', \n",
    "                              task, \n",
    "                              plane, \n",
    "                              transform=None, \n",
    "                              train=train, \n",
    "                              normalize=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=1, \n",
    "                                               shuffle=False, \n",
    "                                               num_workers=10, \n",
    "                                               drop_last=False)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for image, label, _ in tqdm_notebook(train_loader):\n",
    "            logit = mrnet(image.cuda())\n",
    "            prediction = torch.sigmoid(logit)\n",
    "            predictions.append(prediction.item())\n",
    "            labels.append(label.item())\n",
    "\n",
    "    return predictions, labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions computed on train data become new features for training a logistic regression.\n",
    "\n",
    "```python\n",
    "\n",
    "task = 'acl'\n",
    "results = {}\n",
    "\n",
    "for plane in ['axial', 'coronal', 'sagittal']:\n",
    "    predictions, labels = extract_predictions(task, plane)\n",
    "    results['labels'] = labels\n",
    "    results[plane] = predictions\n",
    "    \n",
    "X = np.zeros((len(predictions), 3))\n",
    "X[:, 0] = results['axial']\n",
    "X[:, 1] = results['coronal']\n",
    "X[:, 2] = results['sagittal']\n",
    "\n",
    "y = np.array(labels)\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the logistic regression is trained, we evaluate it on the validation features i.e the models' predictions on validation data.\n",
    "\n",
    "```python\n",
    "task = 'acl'\n",
    "results_val = {}\n",
    "\n",
    "for plane in ['axial', 'coronal', 'sagittal']:\n",
    "    predictions, labels = extract_predictions(task, plane, train=False)\n",
    "    results_val['labels'] = labels\n",
    "    results_val[plane] = predictions\n",
    "\n",
    "y_pred = logreg.predict_proba(X_val)[:, 1]\n",
    "metrics.roc_auc_score(y_val, y_pred)\n",
    "```\n",
    "\n",
    "**Result: we get an AUC of 0.95**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up: where to go from here?\n",
    "\n",
    "You just learnt about ACL tear classification using a deep convolutional neural network.\n",
    "\n",
    "Now is time to check what this model actually learnt. In the next post, we'll investigate an interpretation method that highlights image areas that activate in case of an ACL tear. We'll use this method as a support for validation and augmented diagnosis. It's also a helpful tool that allow radiologists trust machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
